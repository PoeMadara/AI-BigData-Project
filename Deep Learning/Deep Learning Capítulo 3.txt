Chapter 3: Shallow Neural Networks
In the second chapter, we learned about supervised learning using one-dimensional linear regression, but such a model can only represent simple linear relationships between input and output. In this chapter, we will delve into shallow neural networks. These networks can express piecewise linear functions and are powerful enough to approximate relationships between multidimensional inputs and outputs of any complexity.

3.1 Example of Neural Networks
A shallow neural network is a function 
𝑦
=
𝑓
[
𝑥
,
𝜙
]
y=f[x,ϕ] with parameters 
𝜙
ϕ, which maps a multivariable input 
𝑥
x to a multivariable output 
𝑦
y. A comprehensive definition of shallow neural networks will be given in Section 3.4. First, we introduce the core concepts through an example network 
𝑓
[
𝑥
,
𝜙
]
f[x,ϕ]. This network can transform a single variable input 
𝑥
x into a single variable output 
𝑦
y and includes ten parameters 
𝜙
=
{
𝜙
0
,
𝜙
1
,
𝜙
2
,
𝜙
3
,
𝜃
10
,
𝜃
11
,
𝜃
20
,
𝜃
21
,
𝜃
30
,
𝜃
31
}
ϕ={ϕ 
0
​
 ,ϕ 
1
​
 ,ϕ 
2
​
 ,ϕ 
3
​
 ,θ 
10
​
 ,θ 
11
​
 ,θ 
20
​
 ,θ 
21
​
 ,θ 
30
​
 ,θ 
31
​
 }:

𝑦
=
𝑓
[
𝑥
,
𝜙
]
=
𝜙
0
+
𝜙
1
𝑎
[
𝜃
10
+
𝜃
11
𝑥
]
+
𝜙
2
𝑎
[
𝜃
20
+
𝜃
21
𝑥
]
+
𝜙
3
𝑎
[
𝜃
30
+
𝜃
31
𝑥
]
y
​
  
=f[x,ϕ]
=ϕ 
0
​
 +ϕ 
1
​
 a[θ 
10
​
 +θ 
11
​
 x]+ϕ 
2
​
 a[θ 
20
​
 +θ 
21
​
 x]+ϕ 
3
​
 a[θ 
30
​
 +θ 
31
​
 x]
​
  
(3.1)
​
 
This calculation process can be divided into three steps: first, calculate the three linear functions of the input data 
𝑥
x (
𝜃
10
+
𝜃
11
𝑥
,
𝜃
20
+
𝜃
21
𝑥
,
𝜃
30
+
𝜃
31
𝑥
θ 
10
​
 +θ 
11
​
 x,θ 
20
​
 +θ 
21
​
 x,θ 
30
​
 +θ 
31
​
 x). Next, process the results of these three functions through the activation function 
𝑎
[
⋅
]
a[⋅]. Finally, weight the three activation results with 
𝜙
1
,
𝜙
2
,
𝜙
3
ϕ 
1
​
 ,ϕ 
2
​
 ,ϕ 
3
​
 , sum them up, and add an offset 
𝜙
0
ϕ 
0
​
 .

Next, we need to define the activation function 
𝑎
[
⋅
]
a[⋅]. Although there are many choices, the most commonly used is the Rectified Linear Unit (ReLU):

𝑎
[
𝑧
]
=
𝑅
𝑒
𝐿
𝑈
[
𝑧
]
=
{
0
if 
𝑧
<
0
𝑧
if 
𝑧
≥
0
(3.2)
a[z]=ReLU[z]={ 
0
z
​
  
if z<0
if z≥0
​
 (3.2)
This function returns the input value when the input is positive, otherwise it returns zero (see Figure 3.1).

Equation 3.1 may not clearly describe the type of input/output relationship at first glance. However, all the concepts mentioned in the previous chapter apply here. Equation 3.1 represents a family of functions, and the specific function depends on the ten parameters in 
𝜙
ϕ. If we know these parameters, we can perform inference (predict 
𝑦
y) by calculating this equation for a given input 
𝑥
x. Given a training dataset 
{
𝑥
𝑖
,
𝑦
𝑖
}
𝑖
=
1
𝐼
{x 
i
​
 ,y 
i
​
 } 
i=1
I
​
 , we can define a least squares loss function 
𝐿
[
𝜙
]
L[ϕ] to evaluate how well the model describes this dataset for any parameter value 
𝜙
ϕ. To train this model, we need to find the parameter values 
𝜙
^
ϕ
^
​
  that minimize this loss.

3.1.1 Intuitive Understanding of Neural Networks
In fact, Equation 3.1 describes a family of continuous piecewise linear functions (see Figure 3.2), this family contains up to four linear regions. To better understand, we will break this function into two parts. First, we define several intermediate quantities:

ℎ
1
=
𝑎
[
𝜃
10
+
𝜃
11
𝑥
]
ℎ
2
=
𝑎
[
𝜃
20
+
𝜃
21
𝑥
]
ℎ
3
=
𝑎
[
𝜃
30
+
𝜃
31
𝑥
]
 		
(3.3)
h1=a[θ 
10
​
 +θ 
11
​
 x]
h2=a[θ 
20
​
 +θ 
21
​
 x]
h3=a[θ 
30
​
 +θ 
31
​
 x]
​
 (3.3)
Here, 
ℎ
1
,
ℎ
2
,
ℎ
3
h1,h2,h3 are called hidden units. Then, we calculate the output by combining these hidden units with a linear function:

𝑦
=
𝜙
0
+
𝜙
1
ℎ
1
+
𝜙
2
ℎ
2
+
𝜙
3
ℎ
3
(3.4)
y=ϕ 
0
​
 +ϕ 
1
​
 h1+ϕ 
2
​
 h2+ϕ 
3
​
 h3(3.4)
Figure 3.3 shows the calculation process forming the function in Figure 3.2a. Each hidden unit contains a linear function of the input 
𝜃
0
+
𝜃
1
𝑥
θ 
0
​
 +θ 
1
​
 x, which is truncated by the ReLU function 
𝑎
[
⋅
]
a[⋅] at zero. These three lines intersect at zero, forming three "hinges" in the final output function. Each of these truncated lines is weighted by 
𝜙
1
,
𝜙
2
,
𝜙
3
ϕ 
1
​
 ,ϕ 
2
​
 ,ϕ 
3
​
 . Finally, adding the weighted sums and an offset 
𝜙
0
ϕ 
0
​
  yields the final output.

Finally, we can combine these linear segments, add weights, and include a bias term 
𝜙
0
ϕ 
0
​
 :

𝑦
=
𝜙
0
+
𝜙
1
ℎ
1
+
𝜙
2
ℎ
2
+
𝜙
3
ℎ
3
(3.4)
y=ϕ 
0
​
 +ϕ 
1
​
 h1+ϕ 
2
​
 h2+ϕ 
3
​
 h3(3.4)
Figure 3.3 shows the computational process for the function in Figure 3.2a. Each hidden unit consists of a linear function of the input 
𝜃
0
+
𝜃
1
𝑥
θ 
0
​
 +θ 
1
​
 x, which is truncated by the ReLU function 
𝑎
[
⋅
]
a[⋅] at zero. The three lines intersect at zero, creating three "hinges" in the final output function. The three truncated lines are weighted by 
𝜙
1
,
𝜙
2
,
𝜙
3
ϕ 
1
​
 ,ϕ 
2
​
 ,ϕ 
3
​
 , respectively. Finally, summing these weighted lines and adding a bias 
𝜙
0
ϕ 
0
​
  produces the final output.

3.1.2 Generalizing to Multiple Inputs
So far, we've discussed a simple network with a single input and a single output. Let's now extend this concept to a network with multiple inputs. Suppose we have an input vector 
𝑥
=
[
𝑥
1
,
𝑥
2
,
…
,
𝑥
𝐷
]
𝑇
x=[x 
1
​
 ,x 
2
​
 ,…,x 
D
​
 ] 
T
  with 
𝐷
D dimensions. The network function 
𝑦
=
𝑓
[
𝑥
,
𝜙
]
y=f[x,ϕ] will now include more parameters, and the hidden units will be calculated using all input dimensions.

For example, a network with two inputs 
𝑥
1
x 
1
​
  and 
𝑥
2
x 
2
​
 , and a single output can be written as:

𝑦
=
𝜙
0
+
𝜙
1
𝑎
[
𝜃
10
+
𝜃
11
𝑥
1
+
𝜃
12
𝑥
2
]
+
𝜙
2
𝑎
[
𝜃
20
+
𝜃
21
𝑥
1
+
𝜃
22
𝑥
2
]
+
𝜙
3
𝑎
[
𝜃
30
+
𝜃
31
𝑥
1
+
𝜃
32
𝑥
2
]
(3.5)
y=ϕ 
0
​
 +ϕ 
1
​
 a[θ 
10
​
 +θ 
11
​
 x 
1
​
 +θ 
12
​
 x 
2
​
 ]+ϕ 
2
​
 a[θ 
20
​
 +θ 
21
​
 x 
1
​
 +θ 
22
​
 x 
2
​
 ]+ϕ 
3
​
 a[θ 
30
​
 +θ 
31
​
 x 
1
​
 +θ 
32
​
 x 
2
​
 ](3.5)
This process involves computing the linear combinations of inputs, applying the activation function, and then combining the results linearly to produce the output.

Figure 3.8 shows the processing steps in a network with two inputs 
𝑥
=
[
𝑥
1
,
𝑥
2
]
𝑇
x=[x 
1
​
 ,x 
2
​
 ] 
T
 , three hidden units 
ℎ
1
,
ℎ
2
,
ℎ
3
h 
1
​
 ,h 
2
​
 ,h 
3
​
 , and one output 
𝑦
y. Panels (a)-(c) illustrate the linear functions of the two input variables for each hidden unit, where brightness indicates the function's output. The thin lines represent contour lines. Panels (d)-(f) show the application of the ReLU activation function to each plane, analogous to the "hinges" in Figure 3.3d-f. Panels (g)-(i) demonstrate the weighted combination of these cut planes, and panel (j) adds a bias term determining the overall surface height. The final result is a continuous surface composed of convex piecewise linear polygonal regions.

Figure 3.9 explores the relationship between the number of hidden units and the maximum number of linear regions they can create for different input dimensions 
𝐷
𝑖
=
{
1
,
5
,
10
,
50
,
100
}
D 
i
​
 ={1,5,10,50,100}. As input dimensions increase, the number of linear regions grows rapidly; for example, with 500 hidden units and input dimension 
𝐷
𝑖
=
100
D 
i
​
 =100, the number of linear regions can exceed 
1
0
107
10 
107
  (solid circles). Panel (b) plots the same data as a function of the number of parameters. Solid circles represent models with 
𝐷
=
500
D=500 hidden units, showing 51,001 parameters, which is relatively small by modern standards.

Figure 3.10 illustrates the relationship between input dimensions and the number of linear regions. Panel (a) shows that a single hidden unit in a single input dimension model creates a split point dividing the input axis into two linear regions. Panel (b) demonstrates that two hidden units in a two-dimensional input model use two lines aligned with the coordinate axes to divide the input space into four regions. Panel (c) shows that three hidden units in a three-dimensional input model use three planes aligned with the coordinate axes to divide the input space into eight regions. Extending this logic, a model with 
𝐷
𝑖
D 
i
​
  input dimensions and 
𝐷
𝑖
D 
i
​
  hidden units can use 
𝐷
𝑖
D 
i
​
  hyperplanes to divide the input space into 
2
𝐷
𝑖
2 
D 
i
​
 
  linear regions.

Figure 3.11 visualizes a neural network with three inputs and two outputs, including twenty parameters: fifteen slopes (indicated by arrows) and five biases (not shown).

Figure 3.12 introduces terminology: A shallow network consists of an input layer, a hidden layer, and an output layer. Each layer connects to the next through forward connections (shown by arrows), hence these models are called feed-forward networks. When every variable in each layer connects to every variable in the next layer, we call it a fully connected network. Each connection represents a slope parameter in the underlying equation, called weights. Variables in the hidden layer are called neurons or hidden units. Values entering the hidden units are pre-activations, and values on the hidden units (after applying the ReLU function) are activations.

Figure 3.13 presents activation functions. Panel (a) shows the logistic sigmoid and tanh functions. Panel (b) shows the Leaky ReLU function and Parametric ReLU function with parameter 0.25. Panel (c) shows the SoftPlus function, Gaussian error linear unit, and sigmoid linear unit. Panel (d) shows the exponential linear unit with parameters 0.5 and 1.0. Panel (e) shows the scaled exponential linear unit. Panel (f) shows the Swish function with parameters 0.4, 1.0, and 1.4.

Figure 3.14 describes a network's processing steps for Problem 3.4, which has one input, three hidden units, and one output. Panels (a)-(c) show each hidden unit receiving input from a linear transformation of the original input. The first two hidden units receive the same inputs as in Figure 3.3, but the last one is different.